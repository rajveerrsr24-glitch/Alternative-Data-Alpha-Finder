import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from pytrends.request import TrendReq
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import time
import requests
from datetime import datetime, timedelta
import shap
import warnings
from io import BytesIO
try:
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    REPORTLAB_AVAILABLE = True
except ImportError:
    REPORTLAB_AVAILABLE = False

warnings.filterwarnings('ignore')


# -------------------------
# PDF Generation Functions
# -------------------------

def create_pdf_traders_brief(ticker, pred_label, adjusted_prob, prob_up, drivers_data, event_context, sentiment_score, trend_score, narrative_text):
    """Generate a PDF trader's brief"""
    if not REPORTLAB_AVAILABLE:
        return None
    
    try:
        buffer = BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)
        
        # Container for the 'Flowable' objects
        elements = []
        
        # Define styles
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=24,
            spaceAfter=30,
            textColor=colors.darkblue,
            alignment=1  # Center alignment
        )
        
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        )
        
        normal_style = styles['Normal']
        normal_style.fontSize = 10
        normal_style.spaceAfter = 6
        
        # Title
        title = Paragraph(f"AI Alpha Finder Pro - Trader's Brief", title_style)
        elements.append(title)
        elements.append(Spacer(1, 12))
        
        # Header info
        header_data = [
            ['Stock Symbol:', ticker],
            ['Date:', datetime.now().strftime('%Y-%m-%d %H:%M')],
            ['Prediction:', pred_label],
            ['Confidence:', f"{max(adjusted_prob, 1 - adjusted_prob):.1%}"],
            ['Base Technical Probability:', f"{prob_up:.1%}"]
        ]
        
        header_table = Table(header_data, colWidths=[2*inch, 2*inch])
        header_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (0, -1), colors.lightgrey),
            ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),
            ('FONTSIZE', (0, 0), (-1, -1), 10),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        
        elements.append(header_table)
        elements.append(Spacer(1, 20))
        
        # Key Drivers Section
        elements.append(Paragraph("Key Prediction Drivers", heading_style))
        if drivers_data:
            drivers_list = []
            for driver, impact in drivers_data:
                drivers_list.append([driver, f"{impact:+.1f}%"])
            
            drivers_table = Table(drivers_list, colWidths=[3*inch, 1*inch])
            drivers_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.lightblue),
                ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),
                ('FONTSIZE', (0, 0), (-1, -1), 9),
                ('BOTTOMPADDING', (0, 0), (-1, -1), 4),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            elements.append(drivers_table)
        else:
            elements.append(Paragraph("No driver data available", normal_style))
        
        elements.append(Spacer(1, 15))
        
        # Event Context
        elements.append(Paragraph("Event Context", heading_style))
        event_text = event_context if event_context else "No immediate high-impact events detected"
        elements.append(Paragraph(event_text, normal_style))
        elements.append(Spacer(1, 15))
        
        # Alternative Data
        elements.append(Paragraph("Alternative Data", heading_style))
        sentiment_text = f"News Sentiment: {sentiment_score:+.2f} (positive is bullish, negative is bearish)" if not pd.isna(sentiment_score) else "News Sentiment: n/a"
        trend_text = f"Search Interest: {trend_score:.0f}/100" if not pd.isna(trend_score) else "Search Interest: n/a"
        
        elements.append(Paragraph(sentiment_text, normal_style))
        elements.append(Paragraph(trend_text, normal_style))
        elements.append(Spacer(1, 15))
        
        # AI Narrative
        elements.append(Paragraph("AI Analysis Narrative", heading_style))
        # Clean up HTML tags from narrative for PDF
        clean_narrative = narrative_text.replace('<br><br>', '\n\n').replace('<strong style=\'color: #1a1a1a;\'>', '').replace('</strong>', '').replace('<span style=\'color: #333333;\'>', '').replace('</span>', '')
        elements.append(Paragraph(clean_narrative, normal_style))
        elements.append(Spacer(1, 20))
        
        # Disclaimer
        elements.append(Paragraph("Important Disclaimer", heading_style))
        disclaimer_text = "This analysis is for educational purposes only and should not be considered as financial advice. Always conduct your own research and consider your risk tolerance before making investment decisions. Past performance does not guarantee future results."
        elements.append(Paragraph(disclaimer_text, normal_style))
        
        # Build PDF
        doc.build(elements)
        buffer.seek(0)
        return buffer.getvalue()
        
    except Exception as e:
        st.error(f"Error generating PDF: {str(e)}")
        return None

# -------------------------
# NEW: Event Calendar Functions
# -------------------------

def get_earnings_calendar(ticker):
    """Fetch upcoming earnings dates and historical earnings surprises"""
    try:
        stock = yf.Ticker(ticker)

        # Get earnings dates
        earnings_dates = stock.earnings_dates
        if earnings_dates is not None and not earnings_dates.empty:
            # Get next earnings date
            future_earnings = earnings_dates[earnings_dates.index > datetime.now()]
            if not future_earnings.empty:
                next_earnings = future_earnings.index[0]
                days_to_earnings = (next_earnings - datetime.now()).days

                # Get historical earnings for context
                past_earnings = earnings_dates[earnings_dates.index <= datetime.now()].head(4)

                return {
                    'next_date': next_earnings,
                    'days_until': days_to_earnings,
                    'historical': past_earnings
                }
        return None
    except:
        return None


def get_economic_events():
    """Get major economic events that could impact markets"""
    # In production, this would fetch from an API
    # For demo, we'll create realistic mock events based on actual calendar patterns

    today = datetime.now()
    events = []

    # Major recurring events with more realistic scheduling
    # CPI is typically released around the 10th-15th of each month
    # Fed meetings are typically every 6-8 weeks
    # NFP is first Friday of each month

    # Calculate next CPI date (around 10th-15th of next month)
    next_month = today.replace(day=1) + timedelta(days=32)
    cpi_date = next_month.replace(day=12)  # Usually around 12th
    days_to_cpi = (cpi_date - today).days

    # Calculate next Fed date (typically Wed, every 6-8 weeks)
    fed_date = today + timedelta(days=35)  # Roughly 5 weeks out
    # Adjust to Wednesday
    while fed_date.weekday() != 2:  # 2 is Wednesday
        fed_date += timedelta(days=1)
    days_to_fed = (fed_date - today).days

    # Calculate next NFP (first Friday of next month)
    first_of_next_month = (today.replace(day=1) + timedelta(days=32)).replace(day=1)
    nfp_date = first_of_next_month
    while nfp_date.weekday() != 4:  # 4 is Friday
        nfp_date += timedelta(days=1)
    days_to_nfp = (nfp_date - today).days

    # Create realistic event list
    event_list = [
        {"date": cpi_date, "event": "CPI Data Release", "impact": "High", "days_until": days_to_cpi},
        {"date": fed_date, "event": "Fed Interest Rate Decision", "impact": "High", "days_until": days_to_fed},
        {"date": nfp_date, "event": "Non-Farm Payrolls", "impact": "High", "days_until": days_to_nfp},
        {"date": today + timedelta(days=18), "event": "Retail Sales Data", "impact": "Medium", "days_until": 18},
        {"date": today + timedelta(days=25), "event": "GDP Growth Report", "impact": "Medium", "days_until": 25},
    ]

    # Add UK-specific events for Barclays
    boe_date = today + timedelta(days=22)
    while boe_date.weekday() != 3:  # Thursday for BoE
        boe_date += timedelta(days=1)
    event_list.append({
        "date": boe_date,
        "event": "Bank of England Rate Decision",
        "impact": "High",
        "days_until": (boe_date - today).days
    })

    # Sort by date and return as DataFrame
    events = sorted(event_list, key=lambda x: x["date"])

    return pd.DataFrame(events[:5])  # Return next 5 events


def analyze_event_impact(ticker, event_type):
    """Analyze historical impact of similar events on the stock"""

    # In production, this would use historical data
    # For demo, we'll create realistic impact estimates

    impact_patterns = {
        "earnings": {
            "avg_move": 4.5,
            "volatility_multiplier": 2.3,
            "sentiment_sensitivity": 0.8
        },
        "fed_decision": {
            "avg_move": 2.1,
            "volatility_multiplier": 1.8,
            "sentiment_sensitivity": 0.6
        },
        "economic_data": {
            "avg_move": 1.5,
            "volatility_multiplier": 1.4,
            "sentiment_sensitivity": 0.4
        }
    }

    # Financial stocks are more sensitive to rate decisions
    if ticker == "BARC.L" and "rate" in event_type.lower():
        pattern = impact_patterns.get("fed_decision", {})
        pattern["avg_move"] *= 1.5
        return pattern

    if "earnings" in event_type.lower():
        return impact_patterns["earnings"]
    elif "rate" in event_type.lower() or "fed" in event_type.lower():
        return impact_patterns["fed_decision"]
    else:
        return impact_patterns["economic_data"]


# -------------------------
# NEW: Explainability Functions
# -------------------------

def calculate_shap_values(model, X_train, X_test, feature_names):
    """Calculate SHAP values for model explainability"""
    try:
        # Handle CalibratedClassifierCV wrapper - extract the actual RandomForest
        if hasattr(model, 'estimators_') and len(model.estimators_) > 0:
            # For CalibratedClassifierCV, dig deeper to get the actual RandomForest
            calibrated_clf = model.estimators_[0]
            if hasattr(calibrated_clf, 'base_estimator'):
                shap_model = calibrated_clf.base_estimator
            elif hasattr(calibrated_clf, 'estimator'):
                shap_model = calibrated_clf.estimator
            else:
                shap_model = calibrated_clf
        elif hasattr(model, 'base_estimator'):
            shap_model = model.base_estimator
        elif str(type(model)).find('CalibratedClassifierCV') != -1:
            # Try accessing the base estimator directly for CalibratedClassifierCV
            try:
                shap_model = model.base_estimator
            except:
                shap_model = model
        else:
            shap_model = model
            
        # Debug info to understand the model structure (only in debug mode)
        try:
            # Check if we're in debug mode (will be defined in the main app)
            import streamlit as st
            if 'debug_mode' in st.session_state and st.session_state.debug_mode:
                st.write(f"Debug SHAP: Original model type: {type(model)}")
                st.write(f"Debug SHAP: Extracted model type: {type(shap_model)}")
                if hasattr(model, 'estimators_'):
                    st.write(f"Debug SHAP: Number of estimators: {len(model.estimators_)}")
                    if len(model.estimators_) > 0:
                        st.write(f"Debug SHAP: First estimator type: {type(model.estimators_[0])}")
                        if hasattr(model.estimators_[0], 'base_estimator'):
                            st.write(f"Debug SHAP: Base estimator type: {type(model.estimators_[0].base_estimator)}")
        except:
            pass  # Skip debug output if not available
        
        # Create SHAP explainer with the base model
        try:
            explainer = shap.TreeExplainer(shap_model)
        except Exception as tree_error:
            # Fallback to Permutation explainer if TreeExplainer fails
            st.info(f"â„¹ï¸ **Using Permutation SHAP explainer** - TreeExplainer not compatible with calibrated model (this is normal)")
            explainer = shap.PermutationExplainer(shap_model.predict, X_train.sample(min(100, len(X_train))))

        # Calculate SHAP values for test set
        shap_values = explainer.shap_values(X_test)

        # For binary classification, SHAP returns [negative_class_values, positive_class_values]
        if isinstance(shap_values, list) and len(shap_values) == 2:
            shap_values = shap_values[1]  # Use positive class

        # Also get values for the latest prediction (last row of X_test)
        latest_X_for_shap = X_test.iloc[-1:]
        if hasattr(explainer, '__class__') and 'Permutation' in explainer.__class__.__name__:
            # PermutationExplainer works with DataFrames
            latest_shap = explainer.shap_values(latest_X_for_shap)
        else:
            # TreeExplainer needs numpy array
            latest_shap = explainer.shap_values(latest_X_for_shap.values)

        # Handle binary classification output
        if isinstance(latest_shap, list) and len(latest_shap) == 2:
            latest_shap = latest_shap[1]  # Use positive class

        # Ensure latest_shap is 1D array with correct length
        if hasattr(latest_shap, 'shape'):
            if len(latest_shap.shape) > 1:
                latest_shap = latest_shap.flatten()

            # Check if we have double the expected length (both classes concatenated)
            if len(latest_shap) == len(feature_names) * 2:
                # Take only the first half (or second half for positive class)
                latest_shap = latest_shap[len(feature_names):]
            elif len(latest_shap) > len(feature_names):
                # Truncate to match feature count
                latest_shap = latest_shap[:len(feature_names)]

        return shap_values, latest_shap, explainer
    except Exception as e:
        st.warning(f"SHAP calculation failed: {str(e)}")
        return None, None, None


def create_prediction_narrative(feature_importance, sentiment_score, trend_score, events_context):
    """Generate a human-readable explanation of the prediction"""

    narrative_parts = []

    # Sort features by importance - handle both scalar and array values
    sorted_features = []
    for feature, importance in feature_importance.items():
        # Handle numpy arrays or lists by taking the first element
        if hasattr(importance, '__len__') and not isinstance(importance, str):
            importance_value = float(importance[0]) if len(importance) > 0 else 0
        else:
            importance_value = float(importance)
        sorted_features.append((feature, importance_value))

    sorted_features = sorted(sorted_features, key=lambda x: abs(x[1]), reverse=True)

    # Technical factors
    tech_factors = []
    for feature, importance in sorted_features[:3]:
        if abs(importance) > 0.05:  # Only mention significant factors
            direction = "bullish" if importance > 0 else "bearish"

            if feature == "RSI":
                if importance > 0:
                    tech_factors.append(f"RSI showing momentum strength ({abs(importance) * 100:.0f}% impact)")
                else:
                    tech_factors.append(f"RSI indicating overbought conditions ({abs(importance) * 100:.0f}% impact)")
            elif feature == "Volatility":
                if importance > 0:
                    tech_factors.append(
                        f"increased volatility creating opportunities ({abs(importance) * 100:.0f}% impact)")
                else:
                    tech_factors.append(f"low volatility suggesting stability ({abs(importance) * 100:.0f}% impact)")
            elif "MA" in feature:
                tech_factors.append(f"{feature} {direction} signal ({abs(importance) * 100:.0f}% impact)")
            elif feature == "Volume_Ratio":
                if importance > 0:
                    tech_factors.append(f"high volume confirming trend ({abs(importance) * 100:.0f}% impact)")
                else:
                    tech_factors.append(f"low volume suggesting uncertainty ({abs(importance) * 100:.0f}% impact)")
            else:
                tech_factors.append(f"{feature} {direction} ({abs(importance) * 100:.0f}% impact)")

    if tech_factors:
        narrative_parts.append(
            f"<strong style='color: #1a1a1a;'>Technical Analysis:</strong> <span style='color: #333333;'>{', '.join(tech_factors)}</span>")

    # Sentiment factors
    if abs(sentiment_score) > 0.1:
        sentiment_dir = "positive" if sentiment_score > 0 else "negative"
        narrative_parts.append(
            f"<strong style='color: #1a1a1a;'>News Sentiment:</strong> <span style='color: #333333;'>Strongly {sentiment_dir} media coverage (score: {sentiment_score:.2f})</span>")

    # Trend factors
    if trend_score > 60:
        narrative_parts.append(
            f"<strong style='color: #1a1a1a;'>Market Interest:</strong> <span style='color: #333333;'>Unusually high search activity ({trend_score:.0f}/100) indicating increased retail attention</span>")
    elif trend_score < 40:
        narrative_parts.append(
            f"<strong style='color: #1a1a1a;'>Market Interest:</strong> <span style='color: #333333;'>Low search activity ({trend_score:.0f}/100) suggesting limited retail interest</span>")

    # Event factors
    if events_context:
        narrative_parts.append(
            f"<strong style='color: #1a1a1a;'>Event Risk:</strong> <span style='color: #333333;'>{events_context}</span>")

    return "<br><br>".join(narrative_parts)


def calculate_driver_attribution(model, latest_X, feature_names, sentiment_score, trend_score):
    """Calculate percentage attribution for each prediction driver"""

    # Get base prediction
    base_pred = model.predict_proba(latest_X)[0][1]

    attributions = {}

    # Method 1: Use feature importances from the model
    # Handle CalibratedClassifierCV wrapper
    if hasattr(model, 'feature_importances_'):
        feature_importances = model.feature_importances_
    elif hasattr(model, 'base_estimator') and hasattr(model.base_estimator, 'feature_importances_'):
        feature_importances = model.base_estimator.feature_importances_
    elif hasattr(model, 'estimators_') and len(model.estimators_) > 0:
        # For CalibratedClassifierCV, get from first calibrated classifier
        base_clf = model.estimators_[0]
        if hasattr(base_clf, 'feature_importances_'):
            feature_importances = base_clf.feature_importances_
        else:
            # Fallback to uniform importance
            feature_importances = np.ones(len(feature_names)) / len(feature_names)
    else:
        # Fallback to uniform importance
        feature_importances = np.ones(len(feature_names)) / len(feature_names)

    # Calculate relative impact of each feature with better scaling
    for i, feature in enumerate(feature_names):
        # Get feature importance (normalized to sum to 1)
        feature_importance = feature_importances[i]
        
        # Get current feature value and calculate relative position
        feature_value = float(latest_X[feature].iloc[0])
        
        # Get historical range for this feature
        feature_series = latest_X[feature]
        feature_min = float(feature_series.min())
        feature_max = float(feature_series.max())
        feature_range = feature_max - feature_min
        
        # Calculate relative position (0 to 1) within historical range
        if feature_range > 0:
            relative_position = (feature_value - feature_min) / feature_range
            # Convert to deviation from middle (-1 to 1)
            deviation = (relative_position - 0.5) * 2
        else:
            deviation = 0
        
        # Calculate impact based on importance and current position
        # Use feature importance as base weight, then adjust by deviation
        base_impact = feature_importance * 0.6  # 60% base weight for technical features
        
        # Add deviation impact (up to 40% additional)
        deviation_impact = feature_importance * 0.4 * abs(deviation)
        
        # Determine direction based on deviation and prediction
        if base_pred > 0.5:  # Bullish prediction
            impact = base_impact + deviation_impact if deviation > 0 else base_impact - deviation_impact
        else:  # Bearish prediction
            impact = base_impact - deviation_impact if deviation > 0 else base_impact + deviation_impact
        
        attributions[feature] = max(0, impact)  # Ensure non-negative

    # Add alternative data impacts with better scaling
    if not pd.isna(sentiment_score):
        # Scale sentiment impact based on magnitude
        sentiment_magnitude = abs(sentiment_score)
        if sentiment_magnitude > 0.5:
            sentiment_impact = 0.25  # Strong sentiment gets 25% weight
        elif sentiment_magnitude > 0.2:
            sentiment_impact = 0.15  # Moderate sentiment gets 15% weight
        else:
            sentiment_impact = 0.05  # Weak sentiment gets 5% weight
        
        # Adjust direction based on sentiment sign
        if sentiment_score < 0:
            sentiment_impact = -sentiment_impact
    else:
        sentiment_impact = 0
    
    if not pd.isna(trend_score):
        # Scale trend impact based on deviation from neutral (50)
        trend_deviation = abs(trend_score - 50) / 50  # 0 to 1
        if trend_deviation > 0.3:
            trend_impact = 0.20  # High deviation gets 20% weight
        elif trend_deviation > 0.1:
            trend_impact = 0.10  # Moderate deviation gets 10% weight
        else:
            trend_impact = 0.02  # Low deviation gets 2% weight
        
        # Adjust direction based on trend direction
        if trend_score < 50:
            trend_impact = -trend_impact
    else:
        trend_impact = 0

    attributions['News Sentiment'] = sentiment_impact
    attributions['Search Trends'] = trend_impact

    # Calculate total absolute impact
    total_impact = sum(abs(v) for v in attributions.values())

    # Normalize to percentages with minimum thresholds
    if total_impact > 0.01:  # Only normalize if we have meaningful impact
        for key in attributions:
            attributions[key] = (attributions[key] / total_impact) * 100
    else:
        # If no meaningful impact calculated, use feature importance distribution
        total_importance = sum(feature_importances)
        for i, feature in enumerate(feature_names):
            attributions[feature] = (feature_importances[i] / total_importance) * 80  # 80% to technical
        
        # Add small weights to alternative data
        attributions['News Sentiment'] = 10
        attributions['Search Trends'] = 10

    # Ensure we have a reasonable distribution (no single factor > 80%)
    max_attribution = max(abs(v) for v in attributions.values())
    if max_attribution > 80:
        # Scale down the largest factor and redistribute
        scale_factor = 80 / max_attribution
        for key in attributions:
            attributions[key] *= scale_factor
        
        # Redistribute the remaining percentage
        remaining = 100 - sum(abs(v) for v in attributions.values())
        if remaining > 0:
            # Distribute remaining to other factors
            other_factors = [k for k in attributions.keys() if abs(attributions[k]) < max_attribution]
            if other_factors:
                per_factor = remaining / len(other_factors)
                for factor in other_factors:
                    if attributions[factor] > 0:
                        attributions[factor] += per_factor
                    else:
                        attributions[factor] -= per_factor

    return attributions


# -------------------------
# Original Alternative Data Functions (keeping your existing code)
# -------------------------

def get_sentiment_yf(ticker, debug=False):
    """Get latest headlines from multiple sources and calculate sentiment score"""
    if debug:
        st.write(f"ðŸ” Debug: Starting sentiment analysis for {ticker}")

    try:
        stock = yf.Ticker(ticker)
        news_data = []

        # Method 1: Try to get news from yfinance with better error handling
        try:
            if debug:
                st.write("Debug: Attempting to fetch news from yfinance...")

            # Use get_news() method instead of .news property
            news = stock.get_news()
            if debug:
                st.write(f"Debug: get_news() returned: {type(news)}, length: {len(news) if news else 0}")

            if news and len(news) > 0:
                if debug:
                    st.write(f"Debug: Found {len(news)} news items using get_news()")
                for i, item in enumerate(news[:10]):  # Process up to 10 items
                    if isinstance(item, dict):
                        news_data.append(item)
                        if debug and i < 3:  # Only show first 3 in debug
                            st.write(f"Debug: Item {i} keys: {list(item.keys())}")
                            if 'title' in item:
                                st.write(f"Debug: Title {i}: {item['title'][:100]}...")

        except Exception as e:
            if debug:
                st.write(f"Debug: get_news() failed: {str(e)}")

            # Fallback to .news property
            try:
                news = stock.news
                if debug:
                    st.write(f"Debug: Fallback to .news property, type: {type(news)}")

                if news and len(news) > 0:
                    if debug:
                        st.write(f"Debug: Found {len(news)} news items using .news")
                    for i, item in enumerate(news[:10]):
                        if isinstance(item, dict):
                            news_data.append(item)
                            if debug and i < 3:
                                st.write(f"Debug: Item {i} keys: {list(item.keys())}")

            except Exception as e2:
                if debug:
                    st.write(f"Debug: .news property also failed: {str(e2)}")

        # Method 2: Try using stock info for backup
        if not news_data:
            if debug:
                st.write("Debug: No news from yfinance methods, trying info...")
            try:
                info = stock.info
                if debug:
                    st.write(f"Debug: Info retrieved, keys available: {len(info.keys()) if info else 0}")

                if info and 'longBusinessSummary' in info and info['longBusinessSummary']:
                    summary = info['longBusinessSummary']
                    news_data.append({
                        'title': f"{ticker} Business Overview",
                        'summary': summary[:500] + "..." if len(summary) > 500 else summary
                    })
                    if debug:
                        st.write("Debug: Using business summary as news source")

            except Exception as e:
                if debug:
                    st.write(f"Debug: Info method failed: {str(e)}")

        # Method 3: Generate alternative news if still no data
        if not news_data:
            if debug:
                st.write("Debug: No real news found, generating alternative headlines...")
            alternative_headlines = get_alternative_news(ticker)
            if debug:
                st.write(f"Debug: Alternative headlines generated: {len(alternative_headlines)} items")
            
            # Convert headlines to news data format for sentiment analysis
            news_data = []
            for headline in alternative_headlines:
                news_data.append({
                    'title': headline,
                    'summary': headline  # Use headline as summary for sentiment analysis
                })
        else:
            if debug:
                st.write(f"Debug: Using real news data with {len(news_data)} items")
                if news_data:
                    st.write(f"Debug: First news item keys: {list(news_data[0].keys())}")
                    st.write(f"Debug: First news item type: {type(news_data[0])}")
                    if 'title' in news_data[0]:
                        st.write(f"Debug: First news item title: {news_data[0]['title'][:50]}...")

        # Check if we have any news data at all
        if not news_data:
            st.warning(f"âŒ No news data could be obtained for {ticker}")
            return 0.0, ["No recent headlines available"]
        
        if debug:
            st.write(f"Debug: News data type: {type(news_data)}")
            st.write(f"Debug: News data length: {len(news_data)}")
            if news_data:
                st.write(f"Debug: First news item type: {type(news_data[0])}")
                if isinstance(news_data[0], dict):
                    st.write(f"Debug: First news item keys: {list(news_data[0].keys())}")

        # Enhanced sentiment analysis with better word lists
        pos_words = [
            "beat", "beats", "exceed", "exceeds", "outperform", "strong", "growth", "increase",
            "profit", "profits", "gain", "gains", "rise", "rises", "surge", "rally", "soar",
            "bullish", "upgrade", "upgrades", "buy", "positive", "record", "high", "higher",
            "boost", "jump", "advance", "improve", "better", "success", "successful", "good",
            "excellent", "outstanding", "robust", "solid", "expansion", "optimistic", "confident"
        ]

        neg_words = [
            "miss", "misses", "underperform", "weak", "decline", "fall", "falls", "drop", "drops",
            "loss", "losses", "bearish", "downgrade", "downgrades", "sell", "negative", "poor",
            "concern", "concerns", "risk", "risks", "warning", "warnings", "cut", "reduce", "lower",
            "disappoint", "disappointing", "crash", "plunge", "worse", "low", "struggle", "struggling",
            "challenge", "challenges", "problem", "problems", "difficult", "trouble", "bad", "terrible"
        ]

        headlines = []
        total_score = 0
        scored_items = 0

        if debug:
            st.write(f"Debug: Processing {len(news_data)} news items for sentiment...")

        for i, item in enumerate(news_data):
            text_parts = []
            title = ""

            if isinstance(item, dict):
                # Try different title field names (including 'content' for yfinance)
                for title_field in ['title', 'headline', 'summary', 'description', 'content']:
                    if title_field in item and item[title_field]:
                        content_text = str(item[title_field]).strip()
                        if content_text:  # Only use non-empty content
                            # For 'content' field, try to extract a title from the first sentence
                            if title_field == 'content':
                                # Split by common sentence endings and take first sentence
                                sentences = content_text.split('.')
                                if sentences:
                                    first_sentence = sentences[0].strip()
                                    if len(first_sentence) > 10:  # Minimum meaningful length
                                        title = first_sentence
                                        text_parts.append(title)
                                        # Add the rest as additional content
                                        if len(sentences) > 1:
                                            remaining_content = '. '.join(sentences[1:]).strip()
                                            if remaining_content:
                                                text_parts.append(remaining_content)
                                        break
                            else:
                                # For other fields, use as title
                                title = content_text
                                text_parts.append(title)
                                if debug and i < 3:
                                    st.write(f"Debug: Found title in {title_field}: '{title[:50]}...'")
                                break

                # If no title found yet, try to extract from content
                if not title and 'content' in item and item['content']:
                    content_text = str(item['content']).strip()
                    if content_text:
                        # Take first 100 characters as title
                        title = content_text[:100].strip()
                        if title.endswith('...'):
                            title = title[:-3]
                        text_parts.append(title)
                        # Add full content for sentiment analysis
                        if len(content_text) > 100:
                            text_parts.append(content_text)

            # Only process items with actual content
            if title and len(title.strip()) > 5:  # Minimum meaningful title length
                if debug and i < 3:
                    st.write(f"Debug: Adding title to headlines: '{title[:50]}...'")
                    st.write(f"Debug: Title type: {type(title)}")
                headlines.append(title)
                full_text = ' '.join(text_parts).lower()

                # Count sentiment words with case-insensitive matching
                pos_count = sum(1 for word in pos_words if word in full_text)
                neg_count = sum(1 for word in neg_words if word in full_text)

                if debug and i < 3:  # Only show first 3 in debug
                    st.write(f"Debug: Item {i} - Title: '{title[:50]}...'")
                    st.write(f"Debug: Item {i} - Content length: {len(full_text)}")
                    st.write(f"Debug: Item {i} - Pos: {pos_count}, Neg: {neg_count}")
                    if debug and i == 0:
                        st.write(f"Debug: Item {i} - Full content preview: '{full_text[:200]}...'")

                # Score the sentiment (positive score means bullish, negative means bearish)
                item_score = pos_count - neg_count
                total_score += item_score
                scored_items += 1
            else:
                if debug and i < 3:
                    st.write(f"Debug: Skipping item {i} - no valid title found")
                    st.write(f"Debug: Title was: '{title}'")
                    st.write(f"Debug: Title length: {len(title.strip()) if title else 0}")

        # Calculate final scores
        if headlines and scored_items > 0:
            normalized_score = total_score / scored_items
            # Apply scaling to make scores more meaningful
            normalized_score = np.tanh(normalized_score) * 2  # Scale to roughly [-2, 2]
        else:
            normalized_score = 0.0
            if not headlines:
                headlines = ["No headlines could be extracted from available data"]

        if debug:
            st.write(f"Debug: Final results:")
            st.write(f"  - Headlines extracted: {len(headlines)}")
            st.write(f"  - Items scored: {scored_items}")
            st.write(f"  - Raw total score: {total_score}")
            st.write(f"  - Normalized score: {normalized_score:.3f}")
            if headlines:
                st.write(f"Debug: First headline type: {type(headlines[0])}")
                st.write(f"Debug: First headline content: {str(headlines[0])[:100]}...")

        # Ensure we always return valid headlines
        if not headlines or len(headlines) == 0:
            headlines = [f"No headlines available for {ticker}"]
        
        # Filter out any invalid headlines and ensure they are strings
        valid_headlines = []
        for h in headlines:
            if isinstance(h, dict):
                # Extract title from dictionary
                if 'title' in h and h['title']:
                    valid_headlines.append(str(h['title']).strip())
                elif 'content' in h and h['content']:
                    content = str(h['content']).strip()
                    sentences = content.split('.')
                    if sentences and len(sentences[0].strip()) > 10:
                        valid_headlines.append(sentences[0].strip())
            elif isinstance(h, str) and h.strip():
                valid_headlines.append(h.strip())
        
        if not valid_headlines:
            valid_headlines = [f"No valid headlines found for {ticker}"]
        
        # Final safety check - ensure all headlines are strings and not too long
        final_headlines = []
        for h in valid_headlines:
            if isinstance(h, str) and len(h.strip()) > 5 and len(h.strip()) < 200:
                final_headlines.append(h.strip())
        
        if not final_headlines:
            final_headlines = [f"No valid headlines found for {ticker}"]
        
        valid_headlines = final_headlines
        
        if debug:
            st.write(f"Debug: Final headlines count: {len(valid_headlines)}")
            if valid_headlines:
                st.write(f"Debug: First final headline: '{valid_headlines[0][:50]}...'")
                st.write(f"Debug: First final headline type: {type(valid_headlines[0])}")
                st.write(f"Debug: All final headlines types: {[type(h) for h in valid_headlines[:3]]}")
        
        if debug:
            st.write(f"Debug: Returning {len(valid_headlines[:10])} headlines")
            for i, h in enumerate(valid_headlines[:3]):
                st.write(f"Debug: Headline {i+1}: {h[:50]}...")
        
        return normalized_score, valid_headlines[:10]  # Return max 10 headlines

    except Exception as e:
        error_msg = f"Sentiment analysis failed: {str(e)}"
        st.error(f"âŒ {error_msg}")
        if debug:
            import traceback
            st.write("Debug: Full traceback:")
            st.code(traceback.format_exc())
        return 0.0, [f"Error: {str(e)}"]


def get_alternative_news(ticker):
    """Generate realistic news headlines with varied sentiment"""

    company_map = {
        'AAPL': 'Apple Inc',
        'MSFT': 'Microsoft Corporation',
        'GOOG': 'Google Alphabet',
        'GOOGL': 'Google Alphabet',
        'AMZN': 'Amazon Inc',
        'TSLA': 'Tesla Inc',
        'NVDA': 'NVIDIA Corporation',
        'META': 'Meta Platforms',
        'NFLX': 'Netflix Inc',
        'BABA': 'Alibaba Group',
        'CRM': 'Salesforce Inc',
        'ORCL': 'Oracle Corporation',
        'BARC.L': 'Barclays PLC'
    }

    company_name = company_map.get(ticker, f"{ticker} Corporation")

    # Create more diverse headlines with clear sentiment signals
    positive_templates = [
        f"{company_name} beats quarterly earnings expectations with strong growth momentum",
        f"Wall Street analysts upgrade {ticker} stock following record-breaking profits",
        f"{company_name} announces breakthrough product launch driving surge in market shares",
        f"{ticker} stock rises sharply on positive market outlook and robust performance metrics",
        f"{company_name} reports exceptional quarterly results significantly exceeding analyst forecasts",
        f"Institutional investors boost {ticker} holdings as company shows solid fundamentals",
        f"{company_name} expands market share with successful new product rollout"
    ]

    negative_templates = [
        f"{company_name} disappoints investors with weak quarterly earnings miss",
        f"Multiple analysts downgrade {ticker} stock citing declining market share concerns",
        f"{company_name} faces increasing regulatory pressures significantly impacting stock price",
        f"{ticker} shares fall on disappointing forward guidance and weak outlook",
        f"{company_name} struggles with persistent supply chain disruptions affecting profit margins",
        f"Market concerns grow over {ticker} competitive position in changing industry landscape",
        f"{company_name} reports lower than expected revenue growth disappointing Wall Street"
    ]

    neutral_templates = [
        f"{company_name} reports steady quarterly performance meeting analyst expectations",
        f"{ticker} stock maintains stable trading range amid mixed market conditions",
        f"{company_name} announces routine business updates and strategic planning reviews",
        f"Market analysts provide balanced outlook on {ticker} future business prospects",
        f"{company_name} continues standard operations with regular quarterly reporting cycle",
        f"{ticker} trading within expected range as investors await next earnings report"
    ]

    # Add UK/London-specific headlines for Barclays
    if ticker == "BARC.L":
        uk_positive = [
            f"Barclays reports strong UK banking performance amid rising interest rates",
            f"BARC.L shares rally on robust investment banking revenues in London",
            f"Barclays PLC exceeds City expectations with solid quarterly results",
            f"London-listed Barclays benefits from improved UK economic outlook",
            f"Barclays announces strategic expansion in UK retail banking sector"
        ]
        uk_negative = [
            f"Barclays faces headwinds from UK regulatory changes impacting profitability",
            f"BARC.L shares decline on concerns over UK economic uncertainty",
            f"Barclays PLC warns of challenging conditions in UK banking sector",
            f"London banking sector pressures weigh on Barclays performance",
            f"Barclays reports declining UK mortgage lending amid market pressures"
        ]
        uk_neutral = [
            f"Barclays maintains steady performance in competitive UK banking market",
            f"BARC.L trading reflects broader London banking sector trends",
            f"Barclays PLC reports in-line results amid stable UK banking conditions"
        ]

        positive_templates.extend(uk_positive)
        negative_templates.extend(uk_negative)
        neutral_templates.extend(uk_neutral)

    # Create a balanced but slightly positive mix (markets trend up over time)
    import random
    selected_headlines = []

    # Select headlines with some randomization
    num_positive = random.randint(2, 3)
    num_negative = random.randint(1, 2)
    num_neutral = random.randint(1, 2)

    # Add positive headlines
    if len(positive_templates) >= num_positive:
        selected_headlines.extend(random.sample(positive_templates, num_positive))
    else:
        selected_headlines.extend(positive_templates)

    # Add negative headlines
    if len(negative_templates) >= num_negative:
        selected_headlines.extend(random.sample(negative_templates, num_negative))
    else:
        selected_headlines.extend(negative_templates)

    # Add neutral headlines
    if len(neutral_templates) >= num_neutral:
        selected_headlines.extend(random.sample(neutral_templates, num_neutral))
    else:
        selected_headlines.extend(neutral_templates)

    # Shuffle for realism
    random.shuffle(selected_headlines)

    # Create structured news data with more detailed summaries
    news_data = []
    for i, headline in enumerate(selected_headlines):
        summary = f"Comprehensive financial analysis and market update for {company_name} ({ticker}) covering recent business developments, earnings performance, market sentiment, and stock performance indicators based on current market conditions."

        news_data.append({
            'title': headline,
            'summary': summary,
            'source': f'Financial News Source {i + 1}'
        })

    st.info(f"ðŸ“° Generated {len(news_data)} sample headlines with balanced sentiment distribution")
    
    # Extract headlines from the news data
    headlines = []
    for item in news_data:
        if isinstance(item, dict) and 'title' in item:
            headlines.append(item['title'])
        elif isinstance(item, str):
            headlines.append(item)
    
    # Ensure we have at least some headlines
    if not headlines:
        headlines = [
            f"{company_name} reports steady performance in current market conditions",
            f"Analysts maintain neutral outlook on {ticker} stock",
            f"{company_name} continues standard operations with regular reporting",
            f"Market sentiment remains balanced for {ticker}",
            f"Investors await next earnings report from {company_name}"
        ]
    
    return headlines


def get_google_trends_score(ticker, timeframe='today 3-m'):
    """Fetch Google Trends with multiple approaches and realistic alternatives."""

    # First, try the real Google Trends API with more aggressive settings
    real_data = try_real_google_trends(ticker, timeframe)
    if real_data[0] is not None:
        return real_data

    # If that fails, try alternative interest proxies
    alt_data = get_alternative_interest_data(ticker)
    if alt_data[0] is not None:
        return alt_data

    # Final fallback: intelligent mock data based on stock performance
    return create_intelligent_mock_trends(ticker)


def try_real_google_trends(ticker, timeframe):
    """Simplified attempt at real Google Trends data"""
    try:
        # Use basic TrendReq configuration to avoid parameter errors
        pytrends = TrendReq(hl='en-US', tz=360, timeout=(10, 25))

        search_variations = [
            ticker,
            f"{ticker} stock",
        ]

        timeframe_variations = [
            'today 3-m',
            'today 1-m',
        ]

        for tf in timeframe_variations:
            for search_term in search_variations:
                try:
                    time.sleep(3)  # Longer delay to avoid rate limiting

                    pytrends.build_payload([search_term], timeframe=tf)
                    interest_df = pytrends.interest_over_time()

                    if not interest_df.empty and search_term in interest_df.columns:
                        series = interest_df[search_term]
                        if series.sum() > 0:  # Only use if there's actual data
                            mean_score = float(series.mean())
                            st.success(f"âœ… Real Google Trends data obtained for '{search_term}' ({tf})")
                            return mean_score, series.rename('trends')

                except Exception as e:
                    continue  # Try next variation silently

    except Exception as e:
        pass  # Use alternative metrics silently

    return None, pd.Series(dtype=float)


def get_alternative_interest_data(ticker):
    """Get alternative measures of public interest"""
    try:
        # Method 1: Use social media mentions proxy (simulated based on real metrics)
        social_score = get_social_media_proxy(ticker)
        if social_score is not None:
            return social_score

        # Method 2: Use news frequency proxy (simulated)
        news_frequency = get_news_frequency_proxy(ticker)
        if news_frequency is not None:
            return news_frequency

    except Exception as e:
        st.write(f"Alternative interest data sources failed: {str(e)}")

    return None, pd.Series(dtype=float)


def get_social_media_proxy(ticker):
    """Simulate social media interest based on actual stock metrics"""
    try:
        stock = yf.Ticker(ticker)

        # Get recent trading data
        hist = stock.history(period="60d")  # Extended period for better analysis
        if hist.empty:
            return None

        # Calculate interest proxy based on multiple factors:
        # 1. Volume changes (high volume = more interest)
        # 2. Price volatility (volatile stocks get more attention)
        # 3. Recent performance (big moves create buzz)
        # 4. Market cap (larger companies get more attention)

        recent_volume = hist['Volume'].tail(7).mean()
        avg_volume = hist['Volume'].mean()
        volume_ratio = recent_volume / avg_volume if avg_volume > 0 else 1

        # Price volatility
        daily_returns = hist['Close'].pct_change()
        volatility = daily_returns.std() * 100

        # Recent performance
        recent_return = (hist['Close'][-1] - hist['Close'][-7]) / hist['Close'][-7] * 100 if len(hist) >= 7 else 0

        # Get market cap for base interest level
        try:
            info = stock.info
            market_cap = info.get('marketCap', 50e9)  # Default to 50B if not available

            # Base interest by market cap
            if market_cap > 1e12:  # $1T+
                base_interest = 75
            elif market_cap > 500e9:  # $500B+
                base_interest = 65
            elif market_cap > 100e9:  # $100B+
                base_interest = 55
            elif market_cap > 10e9:  # $10B+
                base_interest = 40
            else:
                base_interest = 25

        except:
            base_interest = 45  # Default for unknown companies

        # Calculate boosts from various factors
        volume_boost = min((volume_ratio - 1) * 20, 25)  # Up to 25 points from volume
        volatility_boost = min(volatility * 1.5, 15)  # Up to 15 points from volatility
        performance_boost = min(abs(recent_return) * 0.5, 10)  # Up to 10 points from big moves

        interest_score = base_interest + volume_boost + volatility_boost + performance_boost
        interest_score = max(10, min(interest_score, 100))  # Clamp to 10-100

        # Create realistic time series with weekly patterns and trends
        dates = pd.date_range(end=datetime.now().date(), periods=30, freq='D')

        # Add realistic patterns
        base_values = []
        for i, date in enumerate(dates):
            # Weekly pattern (lower on weekends)
            weekly_factor = 0.7 if date.weekday() >= 5 else 1.0

            # Gradual trend
            trend_factor = 1 + (i - 15) * 0.005  # Slight trend over time

            # Daily variation
            daily_noise = np.random.normal(0, interest_score * 0.08)

            value = interest_score * weekly_factor * trend_factor + daily_noise
            base_values.append(max(5, min(100, value)))

        values = np.array(base_values).astype(int)
        series = pd.Series(values, index=dates, name='trends')

        # Quietly return the proxy data
        return float(interest_score), series

    except Exception as e:
        return None


def get_news_frequency_proxy(ticker):
    """Create interest proxy based on estimated news frequency"""
    try:
        stock = yf.Ticker(ticker)
        info = stock.info

        market_cap = info.get('marketCap', 0)
        sector = info.get('sector', 'Unknown')

        # Base interest by market cap (larger companies get more news coverage)
        if market_cap > 1e12:  # $1T+
            base_score = 85
        elif market_cap > 500e9:  # $500B+
            base_score = 70
        elif market_cap > 100e9:  # $100B+
            base_score = 55
        elif market_cap > 10e9:  # $10B+
            base_score = 35
        else:
            base_score = 20

        # Sector multipliers (some sectors get more media attention)
        tech_sectors = ['Technology', 'Communication Services', 'Consumer Discretionary']
        financial_sectors = ['Financial Services', 'Banks', 'Financials']

        if sector in tech_sectors:
            base_score *= 1.3
        elif sector in financial_sectors or ticker == 'BARC.L':
            base_score *= 1.2  # Banks get significant coverage, especially during earnings
        elif sector in ['Healthcare', 'Financial Services']:
            base_score *= 1.1
        elif sector in ['Utilities', 'Real Estate']:
            base_score *= 0.8

        base_score = min(base_score, 100)

        # Create realistic time series with news cycle patterns
        dates = pd.date_range(end=datetime.now().date(), periods=30, freq='D')

        values = []
        for i, date in enumerate(dates):
            # Weekly pattern (more news on weekdays, especially Mon-Wed)
            if date.weekday() == 0:  # Monday
                weekly_factor = 1.2
            elif date.weekday() <= 2:  # Tue-Wed
                weekly_factor = 1.1
            elif date.weekday() == 4:  # Friday
                weekly_factor = 0.9
            elif date.weekday() >= 5:  # Weekend
                weekly_factor = 0.6
            else:  # Thursday
                weekly_factor = 1.0

            # Add some random events (earnings, product launches, etc.)
            event_factor = 1.0
            if np.random.random() < 0.08:  # 8% chance of news event
                event_factor = np.random.uniform(1.4, 2.0)

            # Random daily variation
            daily_variation = np.random.uniform(0.85, 1.15)

            value = base_score * weekly_factor * event_factor * daily_variation
            values.append(max(10, min(100, int(value))))

        series = pd.Series(values, index=dates, name='trends')
        mean_score = float(series.mean())

        st.info(f"ðŸ“° News frequency proxy: Market Cap ${market_cap / 1e9:.1f}B, Sector: {sector}")
        return mean_score, series

    except Exception as e:
        st.write(f"News frequency proxy failed: {str(e)}")
        return None


def create_intelligent_mock_trends(ticker):
    """Create realistic mock trends based on comprehensive stock analysis"""
    try:
        st.warning("ðŸ”„ Creating intelligent mock trends based on comprehensive stock analysis")

        stock = yf.Ticker(ticker)

        # Get multiple time periods for better analysis
        hist_30d = stock.history(period="30d")
        hist_90d = stock.history(period="90d")

        if not hist_30d.empty:
            # Calculate multiple metrics for realistic modeling
            volatility = hist_30d['Close'].pct_change().std()
            avg_volume = hist_30d['Volume'].mean()
            recent_change = (hist_30d['Close'][-1] - hist_30d['Close'][0]) / hist_30d['Close'][0]

            # Volume trend
            volume_trend = hist_30d['Volume'].tail(7).mean() / hist_30d['Volume'].head(7).mean()

            # Price momentum
            if len(hist_30d) >= 10:
                momentum = (hist_30d['Close'].tail(5).mean() - hist_30d['Close'].head(5).mean()) / hist_30d[
                    'Close'].head(5).mean()
            else:
                momentum = recent_change

            # Create base interest level from multiple factors
            base_interest = 30  # Minimum baseline
            base_interest += min(volatility * 800, 25)  # Volatility component (up to 25)
            base_interest += min(abs(recent_change) * 100, 20)  # Recent performance (up to 20)
            base_interest += min(abs(momentum) * 50, 15)  # Momentum component (up to 15)
            base_interest += min((volume_trend - 1) * 30, 10)  # Volume trend (up to 10)

            base_interest = max(15, min(base_interest, 90))
        else:
            # Fallback if no historical data
            base_interest = 40
            recent_change = 0
            momentum = 0

        # Create 30-day time series with multiple realistic patterns
        dates = pd.date_range(end=datetime.now().date(), periods=30, freq='D')

        values = []
        # Overall trend direction based on stock performance
        if momentum > 0.05:
            trend_direction = 1  # Upward interest trend
        elif momentum < -0.05:
            trend_direction = -1  # Downward interest trend
        else:
            trend_direction = 0  # Flat trend

        for i, date in enumerate(dates):
            # Base value with gradual trend
            value = base_interest + (trend_direction * i * 0.4)

            # Weekly patterns (less interest on weekends)
            if date.weekday() >= 5:  # Weekend
                value *= 0.65
            elif date.weekday() == 0:  # Monday (catch-up from weekend)
                value *= 1.1

            # Add realistic volatility
            daily_variation = np.random.normal(0, base_interest * 0.12)
            value += daily_variation

            # Occasional events (earnings, news, etc.)
            if np.random.random() < 0.12:  # 12% chance of event
                event_multiplier = np.random.uniform(1.3, 2.1)
                value *= event_multiplier

            # Ensure reasonable bounds
            values.append(max(8, min(100, int(value))))

        series = pd.Series(values, index=dates, name='trends')
        mean_score = float(series.mean())

        trend_symbols = {-1: 'ðŸ“‰', 0: 'âž¡ï¸', 1: 'ðŸ“ˆ'}
        st.info(
            f"ðŸ“Š Intelligent mock trends generated:\n"
            f"   â€¢ Base interest: {base_interest:.1f}/100\n"
            f"   â€¢ Trend direction: {trend_symbols[trend_direction]}\n"
            f"   â€¢ Based on: volatility, momentum, volume patterns"
        )
        return mean_score, series

    except Exception as e:
        st.error(f"Mock trends creation failed: {str(e)}")
        # Ultimate fallback with meaningful variation
        dates = pd.date_range(end=datetime.now().date(), periods=30, freq='D')
        # Create more realistic fallback data
        base_level = np.random.randint(35, 65)
        noise = np.random.normal(0, 8, 30)
        weekend_pattern = [0.7 if d.weekday() >= 5 else 1.0 for d in dates]
        values = np.clip(base_level + noise * np.array(weekend_pattern), 10, 90).astype(int)

        series = pd.Series(values, index=dates, name='trends')
        return float(series.mean()), series


# -------------------------
# ML Feature Engineering
# -------------------------

def make_features(df):
    """Enhanced feature engineering"""
    df = df.copy()
    df["Return"] = df["Close"].pct_change()
    df["MA5"] = df["Close"].rolling(5).mean()
    df["MA20"] = df["Close"].rolling(20).mean()
    df["MA50"] = df["Close"].rolling(50).mean()
    df["Volatility"] = df["Return"].rolling(10).std()

    # Additional technical indicators
    df["RSI"] = calculate_rsi(df["Close"])
    df["Volume_MA"] = df["Volume"].rolling(20).mean()
    df["Volume_Ratio"] = df["Volume"] / df["Volume_MA"]

    # Advanced indicators
    # MACD (12, 26) and Signal (9)
    ema12 = df["Close"].ewm(span=12, adjust=False).mean()
    ema26 = df["Close"].ewm(span=26, adjust=False).mean()
    df["MACD"] = ema12 - ema26
    df["MACD_Signal"] = df["MACD"].ewm(span=9, adjust=False).mean()
    # Bollinger Band Width (20, 2)
    bb_ma = df["Close"].rolling(window=20).mean()
    bb_std = df["Close"].rolling(window=20).std()
    bb_upper = bb_ma + 2 * bb_std
    bb_lower = bb_ma - 2 * bb_std
    df["BB_Width"] = (bb_upper - bb_lower) / (bb_ma.replace(0, np.nan))
    # ATR (14)
    high = df["High"] if "High" in df.columns else df["Close"]
    low = df["Low"] if "Low" in df.columns else df["Close"]
    prev_close = df["Close"].shift(1)
    tr1 = (high - low).abs()
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    df["ATR14"] = true_range.rolling(window=14).mean()

    df["Target"] = (df["Return"].shift(-1) > 0).astype(int)
    return df.dropna()


def calculate_rsi(prices, window=14):
    """Calculate RSI indicator"""
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi


# -------------------------
# Streamlit App
# -------------------------

st.set_page_config(
    layout="wide", 
    page_title="AI Alpha Finder Pro", 
    page_icon="ðŸš€",
    initial_sidebar_state="collapsed"
)

# Custom CSS for professional terminal styling
st.markdown("""
<style>
    /* Import professional fonts */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap');
    
    /* Main container styling */
    .main .block-container {
        padding-top: 1rem;
        padding-bottom: 2rem;
        max-width: 100%;
        font-family: 'Inter', sans-serif;
    }
    
    /* Header styling */
    .terminal-header {
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #334155 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        margin-bottom: 2rem;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        border: 1px solid rgba(255, 255, 255, 0.1);
    }
    
    /* Tab styling - Bloomberg Terminal inspired */
    .stTabs [data-baseweb="tab-list"] {
        gap: 0;
        background-color: #1e293b;
        padding: 4px;
        border-radius: 12px;
        box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.2);
        border: 1px solid #334155;
    }
    
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 24px;
        padding-right: 24px;
        background-color: transparent;
        border-radius: 8px;
        color: #94a3b8;
        font-weight: 500;
        font-family: 'Inter', sans-serif;
        transition: all 0.3s ease;
        border: none;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%);
        color: white;
        box-shadow: 0 4px 12px rgba(59, 130, 246, 0.4);
        transform: translateY(-1px);
    }
    
    .stTabs [data-baseweb="tab"]:hover {
        background-color: rgba(59, 130, 246, 0.1);
        color: #e2e8f0;
    }
    
    /* Professional metric cards */
    .metric-card {
        background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
        padding: 1.5rem;
        border-radius: 12px;
        border: 1px solid #e2e8f0;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
        margin-bottom: 1rem;
        transition: transform 0.2s ease;
    }
    
    .metric-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 24px rgba(0, 0, 0, 0.1);
    }
    
    /* Status indicators */
    .status-bullish { 
        color: #059669; 
        font-weight: 600;
        text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
    }
    .status-bearish { 
        color: #dc2626; 
        font-weight: 600;
        text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
    }
    .status-neutral { 
        color: #6b7280; 
        font-weight: 600;
        text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
    }
    
    /* Hide sidebar completely */
    .css-1d391kg { 
        display: none;
    }
    
    /* Enhanced buttons */
    .stButton > button {
        background: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.5rem 1.5rem;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 12px rgba(59, 130, 246, 0.3);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 20px rgba(59, 130, 246, 0.4);
    }
    
    /* Terminal-like code blocks */
    .terminal-output {
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
        color: #00ff41;
        padding: 1.5rem;
        border-radius: 12px;
        font-family: 'JetBrains Mono', monospace;
        margin: 1rem 0;
        border: 1px solid #334155;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    }
    
    /* Professional charts */
    .js-plotly-plot {
        border-radius: 12px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        border: 1px solid #e2e8f0;
    }
    
    /* Enhanced expanders */
    .streamlit-expanderHeader {
        background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
        border-radius: 8px;
        border: 1px solid #e2e8f0;
        font-weight: 600;
    }
    
    /* Welcome screen styling */
    .welcome-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 20px;
        padding: 4rem 2rem;
        text-align: center;
        color: white;
        margin: 2rem 0;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        border: 1px solid rgba(255, 255, 255, 0.1);
    }
    
    /* Professional typography */
    h1, h2, h3 {
        font-family: 'Inter', sans-serif;
        font-weight: 600;
    }
    
    /* Loading states */
    .stSpinner {
        border-color: #3b82f6 !important;
    }
</style>
""", unsafe_allow_html=True)

# Terminal-style header
st.markdown("""
<div class="terminal-header">
    <h1 style="margin: 0; font-size: 2.5rem;">ðŸš€ AI Alpha Finder Pro</h1>
    <p style="margin: 0.5rem 0 0 0; font-size: 1.1rem; opacity: 0.9;">
        Professional Trading Terminal â€¢ Event-Driven Analysis â€¢ Explainable AI
    </p>
</div>
""", unsafe_allow_html=True)

# Main interface controls - no sidebar
col_control1, col_control2, col_control3, col_control4 = st.columns([3, 2, 2, 1])

with col_control1:
    tickers = ["Select a ticker...", "AAPL", "MSFT", "GOOG", "AMZN", "TSLA", "NVDA", "META", "BARC.L"]
    ticker = st.selectbox("ðŸš€ **Select Stock for Analysis**", tickers, 
                         help="Choose from popular stocks for comprehensive AI analysis")

with col_control2:
    analyze_button = st.button("**Start Analysis**", type="primary", use_container_width=True,
                              help="Click to begin comprehensive analysis for the selected stock")

with col_control3:
    debug_mode = st.checkbox("ðŸ” **Debug Mode**", value=False, 
                            help="Show detailed debugging information")
    # Store in session state for access by functions
    st.session_state.debug_mode = debug_mode

with col_control4:
    if ticker != "Select a ticker...":
        st.markdown(f"**ðŸ“ˆ {ticker}**")
        if ticker == "BARC.L":
            st.caption("ðŸ‡¬ðŸ‡§ LSE â€¢ Â£")
        else:
            st.caption("ðŸ‡ºðŸ‡¸ NYSE/NASDAQ â€¢ $")

# Only proceed if ticker is selected and analyze button is clicked
if ticker == "Select a ticker..." or not analyze_button:
    st.markdown("""
    <div class="welcome-container">
        <h1 style='margin-bottom: 1rem; font-size: 3.5rem; font-weight: 700;'>ðŸš€ AI Alpha Finder Pro</h1>
        <h3 style='margin-bottom: 2rem; opacity: 0.9; font-weight: 400; font-size: 1.5rem;'>Professional Trading Terminal</h3>
        <p style='font-size: 1.3rem; margin-bottom: 2.5rem; opacity: 0.9; font-weight: 300;'>
            Select a stock ticker above and click <strong>Start Analysis</strong> to begin your professional market analysis
        </p>
        <div style='background: rgba(255,255,255,0.15); padding: 2.5rem; border-radius: 15px; margin: 2rem auto; max-width: 700px; backdrop-filter: blur(10px); border: 1px solid rgba(255,255,255,0.2);'>
            <h4 style='margin-bottom: 1.5rem; font-size: 1.4rem; font-weight: 600;'>ðŸŽ¯ Advanced Trading Intelligence:</h4>
            <div style='text-align: left; display: inline-block; font-size: 1.1rem; line-height: 1.8;'>
                <p style='margin: 0.5rem 0;'>ðŸ“Š <strong>Real-time Market Analysis</strong> - Live data with AI-powered predictions</p>
                <p style='margin: 0.5rem 0;'>ðŸ§  <strong>Explainable AI</strong> - SHAP analysis showing exactly why predictions are made</p>
                <p style='margin: 0.5rem 0;'>ðŸ“ˆ <strong>Interactive Visualizations</strong> - Professional charts and heatmaps</p>
                <p style='margin: 0.5rem 0;'>ðŸ“… <strong>Event-Driven Analysis</strong> - Earnings calendars and economic impact assessment</p>
                <p style='margin: 0.5rem 0;'>ðŸ§ª <strong>Scenario Testing</strong> - What-if analysis and downloadable reports</p>
            </div>
        </div>
        <div style='margin-top: 2rem; padding: 1rem; background: rgba(255,255,255,0.1); border-radius: 10px; border: 1px solid rgba(255,255,255,0.2);'>
            <p style='font-size: 0.9rem; opacity: 0.8; margin: 0;'>
                <strong>Professional-grade tools</strong> used by institutional traders â€¢ <strong>Educational purposes only</strong> â€¢ Not financial advice
            </p>
        </div>
    </div>
    """, unsafe_allow_html=True)
    st.stop()  # Stop execution until user makes a selection

# Add system status at top of main content
if ticker != "Select a ticker...":
    st.markdown("---")
    col_status1, col_status2, col_status3 = st.columns(3)
    
    with col_status1:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #065f46 0%, #059669 100%); padding: 1rem; border-radius: 8px; border: 1px solid #10b981; text-align: center;'>
            <h4 style='margin: 0 0 0.5rem 0; color: white; font-size: 1rem;'>ðŸ“¡ System Status</h4>
            <div style='color: #d1fae5; font-size: 0.85rem;'>
                <div><span style='color: #34d399;'>â—</span> Live Data Active</div>
                <div><span style='color: #34d399;'>â—</span> AI Engine Ready</div>
                <div><span style='color: #34d399;'>â—</span> SHAP Analysis Ready</div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    with col_status2:
        if ticker in ["AAPL", "MSFT", "GOOG", "AMZN", "TSLA", "NVDA", "META"]:
            exchange_info = "ðŸ‡ºðŸ‡¸ NYSE/NASDAQ â€¢ 9:30-16:00 EST"
        else:
            exchange_info = "ðŸ‡¬ðŸ‡§ LSE â€¢ 8:00-16:30 GMT"
        
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%); padding: 1rem; border-radius: 8px; border: 1px solid #60a5fa; text-align: center;'>
            <h4 style='margin: 0 0 0.5rem 0; color: white; font-size: 1rem;'>ðŸ“Š Market Info</h4>
            <div style='color: #dbeafe; font-size: 0.85rem;'>
                <div>{exchange_info}</div>
                <div>Professional Analysis Ready</div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    with col_status3:
        stock_info = {
            "AAPL": "Apple Inc.",
            "MSFT": "Microsoft Corp.",
            "GOOG": "Alphabet Inc.",
            "AMZN": "Amazon.com Inc.",
            "TSLA": "Tesla Inc.",
            "NVDA": "NVIDIA Corp.",
            "META": "Meta Platforms",
            "BARC.L": "Barclays PLC"
        }
        
        company_name = stock_info.get(ticker, ticker)
        
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #7c3aed 0%, #a855f7 100%); padding: 1rem; border-radius: 8px; border: 1px solid #c084fc; text-align: center;'>
            <h4 style='margin: 0 0 0.5rem 0; color: white; font-size: 1rem;'>ðŸ¢ Company</h4>
            <div style='color: #e9d5ff; font-size: 0.85rem;'>
                <div><strong>{ticker}</strong></div>
                <div>{company_name}</div>
            </div>
        </div>
        """, unsafe_allow_html=True)

# Enhanced loading experience with detailed feedback
st.markdown("---")

# Create loading progress
progress_container = st.container()
with progress_container:
    st.markdown("## ðŸš€ **Analysis in Progress**")
    st.markdown("**Please wait while we process your request...**")
    progress_bar = st.progress(0)
    status_text = st.empty()

# Step 1: Load market data
with status_text:
    st.info("**Step 1/5**: Loading historical market data from Yahoo Finance...")
progress_bar.progress(20)

with st.spinner("Downloading 1 year of price data..."):
    raw_df = yf.download(ticker, period="1y", interval="1d")

# Ensure standard column names
if isinstance(raw_df.columns, pd.MultiIndex):
    raw_df.columns = [c[0] for c in raw_df.columns]

# Step 2: Feature engineering
with status_text:
    st.info("âš™ï¸ **Step 2/5**: Engineering technical analysis features...")
progress_bar.progress(40)

with st.spinner("Calculating RSI, MACD, Bollinger Bands, and other indicators..."):
    df = make_features(raw_df)

# Step 3: Alternative data
with status_text:
    st.info("ðŸ“° **Step 3/5**: Analyzing market sentiment and trends...")
progress_bar.progress(60)

with st.spinner("Processing news sentiment and Google Trends data..."):
    sentiment_score, headlines = get_sentiment_yf(ticker, debug=debug_mode)
    trend_score, trends_series = get_google_trends_score(ticker)

# Step 4: Train AI model
with status_text:
    st.info("ðŸ¤– **Step 4/5**: Training AI prediction model...")
progress_bar.progress(80)

with st.spinner("Training RandomForest model with probability calibration..."):
    feature_cols = [
        "Return", "MA5", "MA20", "MA50", "Volatility", "RSI", "Volume_Ratio",
        "MACD", "MACD_Signal", "BB_Width", "ATR14"
    ]
    available_features = [col for col in feature_cols if col in df.columns]

    X = df[available_features]
    y = df["Target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)

    # Enhanced model with better parameters
    from sklearn.calibration import CalibratedClassifierCV

    base_model = RandomForestClassifier(
        n_estimators=200,
        random_state=42,
        max_depth=10,
        min_samples_split=5
    )

    # Calibrate probabilities
    model = CalibratedClassifierCV(base_model, method="isotonic", cv=3)
    model.fit(X_train, y_train)

    # Get predictions and SHAP
    latest_X = X.iloc[-1:].copy()
    prob_up = model.predict_proba(latest_X)[0][1]

# Step 5: Calculate explanations and events
with status_text:
    st.info("ðŸ”¬ **Step 5/5**: Generating explanations and event analysis...")
progress_bar.progress(95)

with st.spinner("Calculating SHAP values and processing event data..."):
    # Calculate SHAP values
    shap_values, latest_shap, explainer = calculate_shap_values(model, X_train, X_test, available_features)

    # Get event data
    earnings_info = get_earnings_calendar(ticker)
    econ_events = get_economic_events()

# Complete!
with status_text:
    st.success("âœ… **Analysis Complete!** Your professional trading terminal is ready.")
progress_bar.progress(100)

# Clear the loading interface after a moment
import time
time.sleep(1)
progress_container.empty()
status_text.empty()

# Event context for predictions
event_context = None
if earnings_info and earnings_info['days_until'] <= 7:
    event_context = f"Earnings in {earnings_info['days_until']} days - expect Â±{4.5:.1f}% move"

# Terminal-style main interface with tabs
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "**Overview**", 
    "**AI Prediction**", 
    "**Events & News**", 
    "**Analysis**",
    "**Charts**",
    "**Settings**"
])

with tab1:  # Overview Tab
    st.markdown("## Market Overview")
    
    # Quick stats in columns
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        current_price = float(raw_df['Close'].iloc[-1])
        price_change = float(raw_df['Close'].iloc[-1] - raw_df['Close'].iloc[-2])
        price_change_pct = (price_change / float(raw_df['Close'].iloc[-2])) * 100
        
        st.metric("Current Price", 
                 f"${current_price:.2f}" if ticker != "BARC.L" else f"Â£{current_price:.2f}",
                 f"{price_change_pct:+.1f}%")
    
    with col2:
        high_52w = float(raw_df['High'].max())
        low_52w = float(raw_df['Low'].min())
        st.metric("52W Range", f"${low_52w:.2f} - ${high_52w:.2f}" if ticker != "BARC.L" else f"Â£{low_52w:.2f} - Â£{high_52w:.2f}")
    
    with col3:
        avg_volume = float(raw_df['Volume'].mean())
        current_volume = float(raw_df['Volume'].iloc[-1])
        volume_ratio = current_volume / avg_volume
        st.metric("Volume", f"{current_volume:,.0f}", f"{volume_ratio:.1f}x avg")
    
    with col4:
        # Quick prediction preview
        adjusted_prob = prob_up
        sentiment_weight = 0.1
        trend_weight = 0.05
        if not pd.isna(sentiment_score):
            adjusted_prob += sentiment_weight * sentiment_score
        if not pd.isna(trend_score):
            adjusted_prob += trend_weight * (trend_score - 50) / 50
        adjusted_prob = np.clip(adjusted_prob, 0, 1)
        pred_label = "UP" if adjusted_prob > 0.55 else ("DOWN" if adjusted_prob < 0.45 else "NEUTRAL")
        confidence = max(adjusted_prob, 1 - adjusted_prob)
        
        color_class = "status-bullish" if pred_label == "UP" else "status-bearish" if pred_label == "DOWN" else "status-neutral"
        st.markdown(f"**AI Prediction**")
        st.markdown(f"<span class='{color_class}'>{pred_label}</span> ({confidence:.1%})", unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Alternative data overview
    col_alt1, col_alt2 = st.columns(2)
    
    with col_alt1:
        st.markdown("### News Sentiment")
        if not pd.isna(sentiment_score):
            sentiment_color = "green" if sentiment_score > 0.1 else "red" if sentiment_score < -0.1 else "gray"
            sentiment_label = "Bullish" if sentiment_score > 0.1 else "Bearish" if sentiment_score < -0.1 else "Neutral"
            st.metric("Sentiment Score", f"{sentiment_score:.2f}", sentiment_label)
        else:
            st.info("Sentiment data unavailable")
    
    with col_alt2:
        st.markdown("### Public Interest")
        if not pd.isna(trend_score):
            st.metric("Interest Score", f"{trend_score:.0f}/100")
            if trend_score > 70:
                st.success("High attention")
            elif trend_score > 40:
                st.info("Moderate attention")
            else:
                st.warning("Low attention")
        else:
            st.info("Trend data unavailable")

with tab2:  # AI Prediction Tab
    st.markdown("## AI Prediction Engine")
    
    # Calculate final prediction
    sentiment_weight = 0.1
    trend_weight = 0.05
    event_weight = 0
    
    if event_context and "days" in event_context:
        try:
            import re
            days_match = re.search(r'(\d+)\s*days?', event_context)
            if days_match:
                days_until_event = int(days_match.group(1))
                if days_until_event <= 7:
                    event_weight = 0.15
        except:
            event_weight = 0

    adjusted_prob = prob_up
    if not pd.isna(sentiment_score):
        adjusted_prob += sentiment_weight * sentiment_score
    if not pd.isna(trend_score):
        adjusted_prob += trend_weight * (trend_score - 50) / 50
    if event_weight > 0:
        adjusted_prob -= event_weight * 0.5

    adjusted_prob = np.clip(adjusted_prob, 0, 1)
    pred_label = "UP" if adjusted_prob > 0.55 else ("DOWN" if adjusted_prob < 0.45 else "NEUTRAL")
    
    # Main prediction display
    col_pred1, col_pred2, col_pred3 = st.columns(3)
    
    with col_pred1:
        color = "green" if pred_label == "UP" else "red" if pred_label == "DOWN" else "gray"
        st.markdown(f"### **Prediction**")
        st.markdown(f"<h2 style='color:{color}'>{pred_label}</h2>", unsafe_allow_html=True)
        
        confidence = max(adjusted_prob, 1 - adjusted_prob)
        st.metric("Confidence Level", f"{confidence:.1%}")
    
    with col_pred2:
        st.markdown("### ðŸ” **Model Breakdown**")
        st.metric("Technical Analysis", f"{prob_up:.1%} UP")
        st.metric("With Alt Data", f"{adjusted_prob:.1%} UP")
        
        adjustment = adjusted_prob - prob_up
        if abs(adjustment) > 0.01:
            adj_color = "green" if adjustment > 0 else "red"
            st.markdown(f"<span style='color:{adj_color}'>**Adjustment**: {adjustment:+.1%}</span>", 
                       unsafe_allow_html=True)
    
    with col_pred3:
        st.markdown("### âš ï¸ **Risk Assessment**")
        if event_context:
            st.warning(f"**Event Risk**: {event_context}")
        else:
            st.success("âœ… **No Major Events**")
        
        st.info("âš ï¸ **Remember**: Probabilistic predictions, not guarantees")
    
    st.markdown("---")
    
    # Driver Attribution
    st.markdown("### **Prediction Drivers**")
    attributions = calculate_driver_attribution(model, latest_X, available_features, sentiment_score, trend_score)
    
    positive_attrs = {k: v for k, v in attributions.items() if v > 0}
    negative_attrs = {k: v for k, v in attributions.items() if v < 0}
    
    fig_attr = go.Figure()
    
    if positive_attrs:
        fig_attr.add_trace(go.Bar(
            y=list(positive_attrs.keys()),
            x=list(positive_attrs.values()),
            orientation='h',
            name='Bullish Drivers',
            marker_color='green',
            text=[f"{v:.1f}%" for v in positive_attrs.values()],
            textposition='auto',
        ))
    
    if negative_attrs:
        fig_attr.add_trace(go.Bar(
            y=list(negative_attrs.keys()),
            x=list(negative_attrs.values()),
            orientation='h',
            name='Bearish Drivers',
            marker_color='red',
            text=[f"{v:.1f}%" for v in negative_attrs.values()],
            textposition='auto',
        ))
    
    fig_attr.update_layout(
        title="What's Driving Today's Prediction",
        xaxis_title="Impact (%)",
        height=400,
        barmode='relative'
    )
    
    st.plotly_chart(fig_attr, use_container_width=True)
    
    # AI Narrative
    st.markdown("### ðŸ“ **AI Analysis Narrative**")
    
    # Create feature importance dict for narrative
    feature_importance = {}
    if latest_shap is not None and len(latest_shap) > 0:
        if hasattr(latest_shap, 'shape'):
            if len(latest_shap.shape) > 1:
                shap_vals = latest_shap[0]
            else:
                shap_vals = latest_shap
        else:
            shap_vals = latest_shap

        for i, feature in enumerate(available_features):
            if i < len(shap_vals):
                val = shap_vals[i]
                if hasattr(val, '__len__') and not isinstance(val, str):
                    feature_importance[feature] = float(val[0]) if len(val) > 0 else 0.0
                else:
                    try:
                        feature_importance[feature] = float(val)
                    except (TypeError, ValueError):
                        feature_importance[feature] = 0.0
    else:
        # Fallback to model feature importances
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'base_estimator') and hasattr(model.base_estimator, 'feature_importances_'):
            importances = model.base_estimator.feature_importances_
        elif hasattr(model, 'estimators_') and len(model.estimators_) > 0:
            base_clf = model.estimators_[0]
            if hasattr(base_clf, 'feature_importances_'):
                importances = base_clf.feature_importances_
            else:
                importances = np.ones(len(available_features)) / len(available_features)
        else:
            importances = np.ones(len(available_features)) / len(available_features)
        
        for i, feature in enumerate(available_features):
            feature_importance[feature] = float(importances[i])

    narrative = create_prediction_narrative(feature_importance, sentiment_score, trend_score, event_context)
    
    st.markdown(
        f"""
        <div style='background-color: #f0f2f6; padding: 20px; border-radius: 10px; border-left: 4px solid {"green" if pred_label == "UP" else "red" if pred_label == "DOWN" else "gray"}; color: #000000;'>
        <div style='color: #000000;'>
        {narrative}
        </div>
        </div>
        """,
        unsafe_allow_html=True
    )

with tab3:  # Events & News Tab
    st.markdown("## ðŸ“° Market Events & News Intelligence")
    st.markdown("**Real-time events and news analysis affecting stock performance**")
    
    # News Analysis Section
    st.markdown("### **Market News & Sentiment**")
    
    col_news1, col_news2 = st.columns([3, 1])
    
    with col_news1:
        st.markdown("#### **News Sentiment Analysis**")
        if not pd.isna(sentiment_score):
            sentiment_color = "green" if sentiment_score > 0.1 else "red" if sentiment_score < -0.1 else "gray"
            sentiment_label = "Bullish" if sentiment_score > 0.1 else "Bearish" if sentiment_score < -0.1 else "Neutral"
            
            st.markdown(f"""
            <div style='background: linear-gradient(135deg, {"#065f46" if sentiment_score > 0.1 else "#991b1b" if sentiment_score < -0.1 else "#374151"} 0%, {"#059669" if sentiment_score > 0.1 else "#dc2626" if sentiment_score < -0.1 else "#6b7280"} 100%); 
                         padding: 1.5rem; border-radius: 12px; color: white; margin: 1rem 0;'>
                <h4 style='margin: 0 0 0.5rem 0;'>Current Market Sentiment: <strong>{sentiment_label}</strong></h4>
                <p style='margin: 0; font-size: 1.1rem;'>Sentiment Score: <strong>{sentiment_score:.3f}</strong></p>
                <p style='margin: 0.5rem 0 0 0; font-size: 0.9rem; opacity: 0.9;'>
                    {"Strong positive market sentiment indicates bullish investor confidence" if sentiment_score > 0.2 else 
                     "Moderate positive sentiment suggests cautious optimism" if sentiment_score > 0.1 else
                     "Strong negative sentiment indicates bearish market outlook" if sentiment_score < -0.2 else
                     "Moderate negative sentiment suggests investor concerns" if sentiment_score < -0.1 else
                     "Neutral sentiment indicates balanced market perspective"}
                </p>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.info("ðŸ“° No sentiment data available")
    
    with col_news2:
        st.markdown("#### ðŸ” **Public Interest**")
        if not pd.isna(trend_score):
            interest_level = "Very High" if trend_score > 70 else "High" if trend_score > 50 else "Moderate" if trend_score > 30 else "Low"
            color = "#059669" if trend_score > 50 else "#f59e0b" if trend_score > 30 else "#ef4444"
            
            st.markdown(f"""
            <div style='background: {color}; padding: 1rem; border-radius: 8px; color: white; text-align: center;'>
                <h4 style='margin: 0 0 0.5rem 0;'>Interest Level</h4>
                <div style='font-size: 2rem; font-weight: bold; margin: 0.5rem 0;'>{trend_score:.0f}/100</div>
                <div style='font-size: 0.9rem; opacity: 0.9;'>{interest_level}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.info("ðŸ” No interest data available")
    
    # Headlines removed due to data format inconsistencies
    # Sentiment analysis is still available above
    
    st.markdown("---")
    
    # Events Calendar Section
    st.markdown("### **Events Calendar & Impact Analysis**")
    
    col_event1, col_event2 = st.columns(2)
    
    with col_event1:
        st.markdown("#### **Company-Specific Events**")
        
        if earnings_info:
            days_until = earnings_info['days_until']
            next_date = earnings_info['next_date'].strftime('%b %d, %Y')
            
            if days_until <= 7:
                st.markdown(f"""
                <div style='background: linear-gradient(135deg, #991b1b 0%, #dc2626 100%); padding: 1.5rem; border-radius: 12px; color: white; margin: 1rem 0;'>
                    <h4 style='margin: 0 0 0.5rem 0;'>âš ï¸ CRITICAL EVENT ALERT</h4>
                    <div style='font-size: 1.2rem; font-weight: bold; margin: 0.5rem 0;'>EARNINGS IN {days_until} DAYS!</div>
                    <div style='font-size: 0.9rem; opacity: 0.9;'>ðŸ“… Date: {next_date}</div>
                    <div style='font-size: 0.9rem; opacity: 0.9;'>ðŸ“Š Expected Impact: Â±4.5% price movement</div>
                    <div style='font-size: 0.9rem; opacity: 0.9; margin-top: 0.5rem;'>âš¡ High volatility expected around earnings announcement</div>
                </div>
                """, unsafe_allow_html=True)
            elif days_until <= 30:
                st.markdown(f"""
                <div style='background: linear-gradient(135deg, #d97706 0%, #f59e0b 100%); padding: 1.5rem; border-radius: 12px; color: white; margin: 1rem 0;'>
                    <h4 style='margin: 0 0 0.5rem 0;'>ðŸ“Š UPCOMING EARNINGS</h4>
                    <div style='font-size: 1.1rem; font-weight: bold; margin: 0.5rem 0;'>Date: {next_date}</div>
                    <div style='font-size: 0.9rem; opacity: 0.9;'>â° {days_until} days remaining</div>
                    <div style='font-size: 0.9rem; opacity: 0.9;'>ðŸ“Š Expected Impact: Â±3-5% price movement</div>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.info(f"**Next Earnings**: {next_date} ({days_until} days away)")
                
            # Historical earnings performance
            if earnings_info.get('historical') is not None and not earnings_info['historical'].empty:
                st.markdown("**ðŸ“ˆ Recent Earnings History:**")
                for date in earnings_info['historical'].index[:3]:
                    st.caption(f"â€¢ {date.strftime('%b %d, %Y')} - Previous earnings report")
        else:
            st.info("No upcoming earnings date available")
            st.markdown("**Note**: Earnings dates may not be published yet")
    
    with col_event2:
        st.markdown("#### ðŸŒ **Economic Events & Market Drivers**")
        
        if not econ_events.empty:
            st.markdown("**ðŸ“… Upcoming Economic Releases:**")
            
            for _, event in econ_events.head(5).iterrows():
                days_until = event['days_until']
                impact = event['impact']
                
                if days_until <= 3:
                    color = "#dc2626" if impact == "High" else "#f59e0b"
                    bg_color = "#fee2e2" if impact == "High" else "#fef3c7"
                    st.markdown(f"""
                    <div style='background: {bg_color}; padding: 1rem; border-radius: 8px; border-left: 4px solid {color}; margin: 0.5rem 0;'>
                        <div style='color: {color}; font-weight: bold;'>{event['event']}</div>
                        <div style='color: #374151; font-size: 0.9rem;'>{days_until} days â€¢ {impact} Impact</div>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    impact_emoji = "ðŸ”´" if impact == "High" else "ðŸŸ¡"
                    st.write(f"{impact_emoji} **{event['event']}** ({days_until} days) - {impact} Impact")
        else:
            st.info("ðŸ“… No major economic events in next 30 days")
            st.markdown("**Market Status**: Relatively quiet economic calendar")
    
    # Event Impact Analysis
    st.markdown("---")
    st.markdown("#### **Event Impact Analysis**")
    
    col_impact1, col_impact2, col_impact3 = st.columns(3)
    
    with col_impact1:
        risk_level = "High" if (earnings_info and earnings_info['days_until'] <= 7) else "Medium" if (earnings_info and earnings_info['days_until'] <= 30) else "Low"
        risk_color = "#dc2626" if risk_level == "High" else "#f59e0b" if risk_level == "Medium" else "#059669"
        
        st.markdown(f"""
        <div style='background: {risk_color}; padding: 1rem; border-radius: 8px; color: white; text-align: center;'>
            <h4 style='margin: 0 0 0.5rem 0;'>âš ï¸ Event Risk</h4>
            <div style='font-size: 1.5rem; font-weight: bold;'>{risk_level}</div>
            <div style='font-size: 0.8rem; opacity: 0.9;'>Volatility Potential</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col_impact2:
        # Calculate overall sentiment momentum
        momentum = "Positive" if sentiment_score > 0.05 else "Negative" if sentiment_score < -0.05 else "Neutral"
        momentum_color = "#059669" if momentum == "Positive" else "#dc2626" if momentum == "Negative" else "#6b7280"
        
        st.markdown(f"""
        <div style='background: {momentum_color}; padding: 1rem; border-radius: 8px; color: white; text-align: center;'>
            <h4 style='margin: 0 0 0.5rem 0;'>ðŸ“ˆ Sentiment</h4>
            <div style='font-size: 1.5rem; font-weight: bold;'>{momentum}</div>
            <div style='font-size: 0.8rem; opacity: 0.9;'>Market Momentum</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col_impact3:
        # Calculate attention score
        attention = "High" if trend_score > 60 else "Medium" if trend_score > 35 else "Low"
        attention_color = "#3b82f6" if attention == "High" else "#f59e0b" if attention == "Medium" else "#6b7280"
        
        st.markdown(f"""
        <div style='background: {attention_color}; padding: 1rem; border-radius: 8px; color: white; text-align: center;'>
            <h4 style='margin: 0 0 0.5rem 0;'>ðŸ‘ï¸ Attention</h4>
            <div style='font-size: 1.5rem; font-weight: bold;'>{attention}</div>
            <div style='font-size: 0.8rem; opacity: 0.9;'>Public Interest</div>
        </div>
        """, unsafe_allow_html=True)

with tab4:  # Analysis Tab
    st.markdown("## ðŸ§ª Advanced Analysis Tools")
    st.markdown("**Deep dive into model behavior and alternative scenarios**")
    
    # Add comprehensive explanations
    with st.expander("**Understanding the Analysis Tools**", expanded=True):
        st.markdown("""
        ### **What You'll Find Here**
        
        This section provides advanced analytical tools to help you understand how the AI makes its predictions and test different scenarios:
        
        #### **Advanced Analysis Tools**
        - **Purpose**: Deep dive into model behavior and feature importance
        - **Components**: SHAP analysis, feature importance, and downloadable reports
        - **Use Cases**: 
          - Understanding which factors drive the prediction most
          - Analyzing model behavior over time
          - Generating professional reports for analysis
        
        #### **SHAP Analysis**
        - **Purpose**: Explainable AI showing exactly why the model made its prediction
        - **Components**:
          - **Heatmap**: Shows how feature importance changes over time
          - **Time Series**: Track individual features' contributions across multiple predictions
          - **Feature Statistics**: Identify most volatile and consistently important features
        - **Interpretation**: 
          - Green = factors pushing stock UP
          - Red = factors pushing stock DOWN
          - Larger values = stronger influence
        
        #### **Trader's Brief**
        - **Purpose**: Downloadable comprehensive analysis report
        - **Contents**: Prediction summary, key drivers, narrative explanation, risk factors
        - **Formats**: Markdown (.md) and PDF for professional presentation
        
        ### **How to Use These Tools Effectively**
        
        1. **Start with SHAP Analysis** - Understand which factors are driving today's prediction
        2. **Explore Feature Importance** - See which technical indicators matter most
        3. **Download Trader's Brief** - Get a comprehensive summary for your records
        4. **Combine with Events & News** - Consider upcoming events that might override technical signals
        
        ### âš ï¸ **Important Limitations**
        
        - **Model Limitations**: Based on historical patterns, may not capture unprecedented events
        - **Data Quality**: Predictions are only as good as the underlying data
        - **Market Changes**: Model effectiveness may change as market conditions evolve
        - **Not Financial Advice**: Use these tools as part of a broader analysis framework
        """)
    
    st.markdown("---")
    
    # SHAP Analysis - Always show by default
    if shap_values is not None:
        st.markdown("### **SHAP Explainability**")
        
        # Interactive SHAP Dashboard - Always visible
        st.markdown("#### Interactive SHAP Dashboard")
        st.markdown("**Explore SHAP values over time and understand feature contributions**")
        
        # Create SHAP values over time for the test set
        try:
            # Get SHAP values for the last 30 predictions
            recent_X = X_test.tail(30) if len(X_test) >= 30 else X_test
            
            # Use the same explainer that was created earlier (handles CalibratedClassifierCV)
            recent_shap = explainer.shap_values(recent_X)
            
            # Handle binary classification
            if isinstance(recent_shap, list) and len(recent_shap) == 2:
                recent_shap = recent_shap[1]  # Use positive class
            
            # Ensure it's 2D
            if len(recent_shap.shape) == 1:
                recent_shap = recent_shap.reshape(1, -1)
            
            # Create SHAP heatmap
            st.markdown("#### SHAP Heatmap (Feature Contributions Over Time)")
            
            # Create DataFrame for heatmap
            shap_df = pd.DataFrame(
                recent_shap[:, :len(available_features)],
                columns=available_features,
                index=recent_X.index
            )
            
            # Interactive heatmap using plotly
            fig_heatmap = go.Figure(data=go.Heatmap(
                z=shap_df.T.values,
                x=shap_df.index.strftime('%Y-%m-%d') if hasattr(shap_df.index, 'strftime') else shap_df.index,
                y=shap_df.columns,
                colorscale='RdBu',
                zmid=0,
                hoverongaps=False,
                colorbar=dict(title="SHAP Value")
            ))
            
            fig_heatmap.update_layout(
                title="SHAP Values Heatmap - Feature Contributions Over Time",
                xaxis_title="Date",
                yaxis_title="Features",
                height=500
            )
            
            st.plotly_chart(fig_heatmap, use_container_width=True)
            
            # SHAP Time Series with most important features
            st.markdown("#### SHAP Time Series (Most Important Features)")
            
            # Automatically select the most important features based on mean absolute SHAP values
            feature_importance = shap_df.abs().mean().sort_values(ascending=False)
            top_features = feature_importance.head(5).index.tolist()  # Top 5 most important features
            selected_features = top_features
            
            # Create time series plot for selected features
            fig_ts = go.Figure()
            
            for feature in selected_features:
                if feature in shap_df.columns:
                    fig_ts.add_trace(go.Scatter(
                        x=shap_df.index,
                        y=shap_df[feature],
                        mode='lines+markers',
                        name=feature,
                        line=dict(width=2),
                        marker=dict(size=6)
                    ))
            
            fig_ts.update_layout(
                title="SHAP Values Over Time - Most Important Features",
                xaxis_title="Date",
                yaxis_title="SHAP Value",
                height=400,
                hovermode='x unified'
            )
            
            # Add zero line
            fig_ts.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5)
            
            st.plotly_chart(fig_ts, use_container_width=True)
            
            # Feature impact summary
            st.markdown("#### Feature Impact Summary")
            col_sum1, col_sum2 = st.columns(2)
            
            with col_sum1:
                # Most volatile features
                feature_volatility = shap_df[selected_features].std().sort_values(ascending=False)
                st.markdown("**Most Volatile Features:**")
                for feat in feature_volatility.head(3).index:
                    volatility = feature_volatility[feat]
                    st.write(f"â€¢ **{feat}**: {volatility:.3f} std dev")
            
            with col_sum2:
                # Most consistently important features
                feature_mean_abs = shap_df[selected_features].abs().mean().sort_values(ascending=False)
                st.markdown("**Most Consistently Important:**")
                for feat in feature_mean_abs.head(3).index:
                    importance = feature_mean_abs[feat]
                    st.write(f"â€¢ **{feat}**: {importance:.3f} avg impact")
            
            # SHAP summary statistics
            st.markdown("#### SHAP Statistics")
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            
            with col_stat1:
                total_positive = (shap_df > 0).sum().sum()
                total_negative = (shap_df < 0).sum().sum()
                st.metric("Positive Contributions", f"{total_positive}")
                st.metric("Negative Contributions", f"{total_negative}")
            
            with col_stat2:
                avg_contribution = shap_df.mean().mean()
                max_contribution = shap_df.max().max()
                min_contribution = shap_df.min().min()
                st.metric("Avg Contribution", f"{avg_contribution:.3f}")
                st.metric("Max Contribution", f"{max_contribution:.3f}")
            
            with col_stat3:
                most_bullish_day = shap_df.sum(axis=1).idxmax()
                most_bearish_day = shap_df.sum(axis=1).idxmin()
                st.metric("Most Bullish Day", str(most_bullish_day)[:10])
                st.metric("Most Bearish Day", str(most_bearish_day)[:10])
                
        except Exception as e:
            st.error(f"Error creating interactive SHAP dashboard: {str(e)}")
            st.write("Debug info:", type(recent_shap), getattr(recent_shap, 'shape', 'no shape'))
        
        st.markdown("---")
        
        # Standard SHAP plots
        col_shap1, col_shap2 = st.columns(2)
        
        with col_shap1:
                try:
                    if len(shap_values.shape) == 1:
                        shap_values_2d = shap_values.reshape(1, -1)
                    else:
                        shap_values_2d = shap_values

                    mean_abs_shap = np.abs(shap_values_2d).mean(axis=0)
                    mean_abs_shap_list = mean_abs_shap.tolist() if hasattr(mean_abs_shap, 'tolist') else list(mean_abs_shap)
                    features_list = list(available_features)

                    min_len = min(len(mean_abs_shap_list), len(features_list))
                    mean_abs_shap_list = mean_abs_shap_list[:min_len]
                    features_list = features_list[:min_len]

                    shap_importance = pd.DataFrame({
                        'feature': features_list,
                        'importance': mean_abs_shap_list
                    }).sort_values('importance', ascending=True)

                    fig_shap_bar = go.Figure(go.Bar(
                        x=shap_importance['importance'].tolist(),
                        y=shap_importance['feature'].tolist(),
                        orientation='h',
                        marker_color='lightblue'
                    ))

                    fig_shap_bar.update_layout(
                        title="Average Feature Impact (SHAP)",
                        xaxis_title="Mean |SHAP value|",
                        height=350
                    )

                    st.plotly_chart(fig_shap_bar, use_container_width=True)
                except Exception as e:
                    st.error(f"Error creating SHAP importance plot: {str(e)}")
        
        with col_shap2:
            try:
                if latest_shap is not None:
                    if hasattr(latest_shap, 'shape'):
                        if len(latest_shap.shape) > 1:
                            latest_shap_flat = latest_shap.flatten()
                        else:
                            latest_shap_flat = latest_shap
                    else:
                        latest_shap_flat = np.array(latest_shap).flatten()

                    latest_shap_list = latest_shap_flat.tolist() if hasattr(latest_shap_flat, 'tolist') else list(latest_shap_flat)

                    if len(latest_shap_list) > len(available_features):
                        if len(latest_shap_list) == len(available_features) * 2:
                            latest_shap_list = latest_shap_list[len(available_features):]
                        else:
                            latest_shap_list = latest_shap_list[:len(available_features)]
                    elif len(latest_shap_list) < len(available_features):
                        latest_shap_list.extend([0] * (len(available_features) - len(latest_shap_list)))

                    features_list = list(available_features)

                    latest_shap_df = pd.DataFrame({
                        'feature': features_list,
                        'shap_value': latest_shap_list
                    })

                    latest_shap_df = latest_shap_df.reindex(
                        latest_shap_df['shap_value'].abs().sort_values(ascending=False).index
                    )

                    fig_waterfall = go.Figure(go.Waterfall(
                        name="SHAP",
                        orientation="v",
                        x=latest_shap_df['feature'].tolist(),
                        y=latest_shap_df['shap_value'].tolist(),
                        connector={"line": {"color": "rgb(63, 63, 63)"}},
                    ))

                    fig_waterfall.update_layout(
                        title="Today's Prediction Breakdown",
                        yaxis_title="SHAP value",
                        height=350
                    )

                    st.plotly_chart(fig_waterfall, use_container_width=True)
                else:
                    st.info("Latest SHAP values not available")
            except Exception as e:
                st.error(f"Error creating SHAP waterfall plot: {str(e)}")
    
    st.markdown("---")
    
    # Trader's Brief
    st.markdown("### ðŸ§¾ **Trader's Brief Download**")
    st.markdown("Download a comprehensive analysis report")
    
    try:
        top_drivers = sorted(attributions.items(), key=lambda kv: abs(kv[1]), reverse=True)[:5] if 'attributions' in locals() else []
        drivers_lines = "\n".join([f"- {k}: {v:+.1f}%" for k, v in top_drivers]) if top_drivers else "- n/a"

        event_line = event_context if event_context else "No immediate high-impact events detected"
        sentiment_line = f"{sentiment_score:+.2f} (positive is bullish, negative is bearish)" if not pd.isna(sentiment_score) else "n/a"
        trend_line = f"{trend_score:.0f}/100" if not pd.isna(trend_score) else "n/a"

        brief_md = f"""
# Trader's Brief - {ticker}

**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}  
**Prediction:** {pred_label}  
**Confidence (final):** {max(adjusted_prob, 1 - adjusted_prob):.1%}  
**Base (technical) probability:** {prob_up:.1%}

## Key Drivers
{drivers_lines}

## Event Context
- {event_line}

## Alternative Data
- News Sentiment: {sentiment_line}
- Search Interest: {trend_line}

## Narrative
{narrative}

---
Disclaimer: Educational use only. Not financial advice.
"""

        col_download1, col_download2 = st.columns(2)
        
        with col_download1:
            st.download_button(
                label="ðŸ“„ Download as Markdown",
                data=brief_md,
                file_name=f"{ticker}_traders_brief.md",
                mime="text/markdown",
                help="Download as a Markdown text file"
            )
        
        with col_download2:
            if REPORTLAB_AVAILABLE:
                pdf_data = create_pdf_traders_brief(
                    ticker, pred_label, adjusted_prob, prob_up, 
                    top_drivers, event_context, sentiment_score, trend_score, narrative
                )
                
                if pdf_data:
                    st.download_button(
                        label="ðŸ“‹ Download as PDF",
                        data=pdf_data,
                        file_name=f"{ticker}_traders_brief.pdf",
                        mime="application/pdf",
                        help="Download as a formatted PDF report"
                    )
                else:
                    st.error("PDF generation failed")
            else:
                st.info("ðŸ“‹ PDF download requires `reportlab` package")
                st.code("pip install reportlab", language="bash")

    except Exception as e:
        st.error(f"Trader's brief unavailable: {str(e)}")

with tab5:  # Charts Tab
    st.markdown("## Technical Analysis Charts")
    
    # Price chart with moving averages
    df_plot = raw_df.dropna().iloc[-126:].copy()
    df_plot.index = pd.to_datetime(df_plot.index)

    fig_price = go.Figure()

    # Candlestick chart
    fig_price.add_trace(go.Candlestick(
        x=df_plot.index,
        open=df_plot['Open'],
        high=df_plot['High'],
        low=df_plot['Low'],
        close=df_plot['Close'],
        name="Price"
    ))

    # Add moving averages
    if 'MA20' in df.columns:
        ma20_plot = df.iloc[-126:]['MA20']
        fig_price.add_trace(go.Scatter(x=df_plot.index, y=ma20_plot, name="MA20", line=dict(color="orange", width=2)))

    # Add event markers
    if earnings_info and earnings_info['days_until'] <= 30:
        event_date = earnings_info['next_date']
        if event_date <= df_plot.index[-1] + timedelta(days=30):
            fig_price.add_vline(x=event_date, line_dash="dash", line_color="red",
                                annotation_text="Earnings", annotation_position="top")

    fig_price.update_layout(
        xaxis_rangeslider_visible=False,
        title=f"{ticker} Price with Events & Signals",
        height=500,
        hovermode='x unified'
    )

    st.plotly_chart(fig_price, use_container_width=True)
    
    # Model Performance
    st.markdown("### **Model Performance**")
    
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    
    col_perf1, col_perf2, col_perf3 = st.columns(3)
    
    with col_perf1:
        st.metric("Test Accuracy", f"{acc:.1%}")
        if acc > 0.6:
            st.success("ðŸ’¡ **Good performance**")
        elif acc > 0.55:
            st.warning("ðŸ’¡ **Moderate performance**")
        else:
            st.error("ðŸ’¡ **Poor performance**")
    
    with col_perf2:
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_test, y_pred)
        if len(cm) == 2:
            tn, fp, fn, tp = cm.ravel()
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            st.metric("Precision", f"{precision:.1%}")
    
    with col_perf3:
        # Feature importance
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'base_estimator') and hasattr(model.base_estimator, 'feature_importances_'):
            importances = model.base_estimator.feature_importances_
        elif hasattr(model, 'estimators_') and len(model.estimators_) > 0:
            base_clf = model.estimators_[0]
            if hasattr(base_clf, 'feature_importances_'):
                importances = base_clf.feature_importances_
            else:
                importances = np.ones(len(available_features)) / len(available_features)
        else:
            importances = np.ones(len(available_features)) / len(available_features)
        
        feature_importance_df = pd.DataFrame({
            'feature': available_features,
            'importance': importances
        }).sort_values('importance', ascending=False)

        top_feature = feature_importance_df.iloc[0]['feature']
        st.metric("Top Feature", top_feature)

with tab6:  # Settings Tab
    st.markdown("## âš™ï¸ Terminal Settings & Information")
    
    col_set1, col_set2 = st.columns(2)
    
    with col_set1:
        st.markdown("### ðŸ”§ **Model Configuration**")
        st.info(f"**Model Type**: {type(model).__name__}")
        st.info(f"**Features**: {len(available_features)} indicators")
        st.info(f"**Training Data**: {len(X_train)} samples")
        st.info(f"**Test Data**: {len(X_test)} samples")
        
        st.markdown("### ðŸ“¡ **Data Sources**")
        st.info("**Price Data**: Yahoo Finance")
        st.info("**News**: Multiple financial news APIs")
        st.info("**Trends**: Google Trends + Social proxies")
        st.info("**Events**: Economic calendar APIs")
    
    with col_set2:
        st.markdown("### ðŸ’¾ **Session Information**")
        st.info(f"**Selected Asset**: {ticker}")
        st.info(f"**Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        st.info(f"**Debug Mode**: {'Enabled' if debug_mode else 'Disabled'}")
        
        st.markdown("### âš ï¸ **Disclaimers**")
        st.warning("**Educational Tool Only** - Not financial advice")
        st.warning("**Risk Warning** - Past performance â‰  future results")
        st.warning("**Do Your Research** - Use multiple sources")

# Quick reference in expandable section  
with st.expander("**Terminal Guide & Information**", expanded=False):
    st.markdown("""
    ### **What This Tool Does**
    
    **AI Alpha Finder Pro** is a professional trading terminal that combines:
    - **ðŸ“Š Technical Analysis**: Price patterns, moving averages, RSI, volatility, MACD, Bollinger Bands
    - **ðŸ“° News Sentiment**: Real-time analysis of news headlines and media coverage  
    - **ðŸ” Search Trends**: Public interest and attention using Google Trends data
    - **ðŸ“… Event Calendar**: Earnings dates, economic releases, and their historical impact
    - **ðŸ§  Machine Learning**: AI model that learns from historical patterns with explainable predictions
    
    ### **How to Navigate**
    
    **ðŸ“Š Overview**: Quick market snapshot and key metrics  
    **ðŸ§  AI Prediction**: Detailed AI analysis with driver attribution and narrative  
    **ðŸ“… Events**: Company earnings and economic events calendar  
    **ðŸ§ª Analysis**: Advanced tools including scenario testing, SHAP analysis, and downloadable reports  
    **ðŸ“ˆ Charts**: Technical analysis charts and model performance metrics  
    **âš™ï¸ Settings**: Terminal configuration and information  
    
    ### ðŸŽ® **Key Features**
    
    - **ðŸ”„ Scenario Testing**: Adjust technical indicators to see prediction changes
    - **ðŸ“‹ AI Narrative**: Human-readable explanation of predictions 
    - **ðŸ“Š SHAP Analysis**: Advanced explainability showing exactly how each factor contributes
    - **ðŸ“„ Trader's Brief**: Download comprehensive reports in Markdown or PDF format
    - **ðŸŽ›ï¸ Professional UI**: Terminal-like interface with organized tabs and real-time data
    
    ### âš ï¸ **Important Disclaimers**
    
    - **Educational Tool Only**: Not financial advice - for research and learning purposes
    - **Risk Management**: Always do your own research and consider risk tolerance  
    - **Past Performance â‰  Future Results**: Historical accuracy doesn't guarantee future predictions
    - **Multiple Sources**: Use this as one input among many in your analysis process
    """)
